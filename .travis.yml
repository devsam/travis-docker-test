sudo: required
dist: trusty
# env:
#   matrix:
#   - OS_TYPE=centos OS_VERSION=7

services:
  - docker

before_install:
  - sudo apt-get update
  - echo 'DOCKER_OPTS="-H tcp://127.0.0.1:2375 -H unix:///var/run/docker.sock -s devicemapper"' | sudo tee /etc/default/docker > /dev/null
  - sudo service docker restart
  - sleep 3
  - docker build -t gaia-pygaia:0.1 .
  - docker run -itd --name gaia --net=host gaia-pygaia:0.1
  - jdk_switcher use oraclejdk8

install:
  - sudo pip install pep8
  - sudo pip install codecov

python : 2.7

env:
  global:
    - TEST_KAFKA_VERSION=2.11-0.11.0.1
    - PYTHONPATH="`pwd`/gaia/src/pygaia:$PYTHONPATH"
  matrix:
    include:
      # Spark 2.1.0
      - env: >
          TEST_SPARK_VERSION=2.1.0 
          SPARK_HOME="`pwd`/thirdparty/spark-$TEST_SPARK_VERSION-bin-hadoop2.7" 
          PYTHONPATH="`pwd`/src/main/python:$SPARK_HOME/python/:$PYTHONPATH"

      # Spark 2.2.0
      - env: >
          TEST_SPARK_VERSION=2.2.0 
          SPARK_HOME=`pwd`/thirdparty/spark-$TEST_SPARK_VERSION-bin-hadoop2.7 
          PYTHONPATH=`pwd`/src/main/python:$SPARK_HOME/python/:$PYTHONPATH

before_script:
  # Download and setup Spark
  - mkdir thirdparty
  - curl -O http://d3kbcqa49mib13.cloudfront.net/spark-$TEST_SPARK_VERSION-bin-hadoop2.7.tgz
  - tar xfz spark-$TEST_SPARK_VERSION-bin-hadoop2.7.tgz --directory thirdparty
  # Download and setup kafka(with zookeeper)
  - curl -O http://mirror.apache-kr.org/kafka/0.11.0.1/kafka_2.11-0.11.0.1.tgz
  - tar xfz kafka_$TEST_KAFKA_VERSION.tgz --directory thirdparty
  - cd kafka_2.11-0.11.0.1 && bin/zookeeper-server-start.sh -daemon config/zookeeper.properties && bin/kafka-server-start.sh -daemon config/server.properties && bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic default
  # TODO: Need consumer start.

script:
  - pep8 --first --max-line-length=99 `find src -name "*.py"`
  - docker cp src/sample.py gaia:/gaia/ && docker exec gaia python /gaia/sample.py
  - echo $SPARK_HOME
  - echo $PYTHONPATH
  - $SPARK_HOME/bin/pyspark --version
 # Run tests in Container
 # - tests/setup_tests.sh ${OS_VERSION}
 # - for f in `find src/pygaia/tests -name "*.py"`;do coverage run -p $f; rc=$?; if [[ $rc != 0 ]]; then exit $rc; fi;done
 # - coverage combine

# after_success:
#   - bash <(curl -s https://codecov.io/bash) -t 5c2e3d60-ed89-49bf-9e53-f8f13e44c294

